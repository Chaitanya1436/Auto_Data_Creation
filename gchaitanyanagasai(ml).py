# -*- coding: utf-8 -*-
"""Gchaitanyanagasai(ML).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RTAmx_KtluLA_wByNqjQVB8M6G6SiKiP
"""

from google.colab import files
uploaded = files.upload()

"""Analyzing The Dataset Features"""

from google.colab import drive
drive.mount('/content/drive')

"""This is how our dataset looks. Once you have identified the input features and the values to be predicted (in our case ‘Purchased’ is the column to be predicted and the rest are the input features) let us analyze the data we have.

"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
dataset= pd.read_csv("/content/drive/MyDrive/Data1.txt")
print(dataset.head(30))

"""This is how our dataset looks. Once you have identified the input features and the values to be predicted (in our case ‘Purchased’ is the column to be predicted and the rest are the input features) let us analyze the data we have."""

x= dataset.iloc[:,1:-1].values
y= dataset.iloc[:,-1].values
print (x)

print(y)



"""Handling Missing Data – An important Feature Engineering Step

A neat way to do that would be to display the sum of all the null values in each column of our dataset. The following line of code helps us do just that.
"""

dataset.isnull().sum()

"""This gives us a very clear representation of the total number of missing values present in each column. Now let us see how we can handle these missing values.

Sometimes there may be certain features in our dataset which contain multiple empty entries or null values. These columns which have a very high number of null values often do not contribute much to the predicted output. In such cases, we may choose to completely delete the column.

We can fix a certain threshold value, say 70% or 80%, and if the number of null values exceeds the threshold we may want to delete that particular column from our training dataset.
"""

threshold=0.7
dataset = dataset[dataset.columns[dataset.isnull().mean() < threshold]]
print(dataset)

"""this piece of code is doing is basically selecting only those columns which have null values less than the given threshold value. In our example, we see that the ‘Cars’ column has been removed. The number of null values is 14 and the total number of entries per column is 20. As the number of null values is not less than our desired threshold, we delete the column."""



"""Imputing Missing Values refers to the process of filling up the missing values with some values computed from the corresponding feature columns.

We can use a number of strategies for Imputing the values of Continuous variables. Some such strategies are imputing with Mean, Median or Mode.
"""

## Let us first display our original variable x.

x= dataset.iloc[:,1:-1].values

y= dataset.iloc[:,-1].values

print(x)

print(y)

"""@@ IMPUTING WITH MEAN @@

Now, to do this, we will import SimpleImputer from sklearn.impute and pass our strategy as the parameter. We shall also specify the columns in which this strategy is to be applied using the slicing.
"""

from sklearn.impute import SimpleImputer
imputer =SimpleImputer(missing_values=np.nan, strategy= "mean")
imputer.fit(x[:,2:3])
x[:,2:3]= imputer.transform(x[:,2:3])

"""@@ IMPUTING WITH MEDIAN @@

Now, to do this, we will import SimpleImputer from sklearn.impute and pass our strategy as the parameter. We shall also specify the columns in which this strategy is to be applied using the slicing.
"""

from sklearn.impute import SimpleImputer
imputer =SimpleImputer(missing_values=np.nan, strategy= "median")
imputer.fit(x[:,2:3])
x[:,2:3]= imputer.transform(x[:,2:3])

"""@@ IMPUTING WITH MODE @@

Now, to do this, we will import SimpleImputer from sklearn.impute and pass our strategy as the parameter. We shall also specify the columns in which this strategy is to be applied using the slicing.
"""

from sklearn.impute import SimpleImputer
imputer =SimpleImputer(missing_values=np.nan, strategy= "most_frequent")
imputer.fit(x[:,1:3])
x[:,1:3]= imputer.transform(x[:,1:3])

print(x)

"""let us modify our dataset a little and another new categorical "Department" which has a few missing entries. This will help us understand how to handle such cases. Our dataset now looks something like this:"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
dataset= pd.read_csv("/content/drive/MyDrive/Data1.txt")
print(dataset.head(30))

"""Checking missing values"""

dataset.isnull().sum()

dataset.head(10)
#lets take only 10 values

"""@@ASSIGNING A NEW CATEGORY TO THE MISSING CATEGORICAL VALUES

Simply deleting the values which are missing, causes loss of information. To avoid that we can also replace the missing values with a new category. For example, we may assign ‘U’ to the missing genders where ‘U’ stands for Unknown.
"""

dataset['Department']= dataset['Department'].fillna('U')
dataset.head(10)

"""Here all the missing values in the dataset['Department']= dataset['Department'].fillna('U')
 column have been replaced with ‘U’. This method adds information to the dataset instead of causing information loss.

We are going to substitute the mode value in the missing fields. Since in our dataset the category with the highest frequency is "ComputerScience", the missing values should be substituted with "ComputerScience".
"""

# Replace 'U' with the mode of the Department column
mode_department = dataset['Department'].mode()[0]
dataset['Department'] = dataset['Department'].replace('U', mode_department)

dataset.head(30)

"""Encoding Independent Variables

Let us get back to our original dataset and have a look at our Independent variable x.
"""

x= dataset.iloc[:,1:-1].values
y= dataset.iloc[:,-1].values
print (x)



"""Let us first understand why this is needed.

Our dataset contains fields like Department which have different names. The Hostler column contains Yes or No. We cannot work with these Categorical variables as they are literals. All these non-numeric values must be encoded into a convenient numeric value that can be used to train our model. This is why we need Encoding of Categorical variables.



Our independent variable x contains a categorical variable Department. This field has different values

So should we encode as 0, 1, 2...?

But Directly


NOOOOOOOO!!!!!!!!

The correct answer is NO. We cannot directly encode the 3 countries as 0,1 and 2. This is because, if we encode the countries in this manner then the machine learning model will wrongly assume that there is some sort of sequential relationship between the countries. This will make the model believe that those are the numbers 0, 1, and 2. This is not true. Hence, we must not feed in the model with such incorrect information.




So what is the solution?

The solution is to create separate columns for each category of the Categorical variable. Then we assign 1 to the column which is true and 0 to the others. The entire set of columns that represent the Categorical variable shall give us the result without creating any ordinal relationship




----This can be done with the help of One Hot Encoding.

The separate columns which are created to represent the categorical variables are known as the Dummy Variables. The fit_transform() method is called from the OneHotEncoder class which creates the dummy variables and assigns them with binary values. Let us have a look at the code.
"""

print(x)

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')
X = np.array(ct.fit_transform(x))
print(X)

"""Encoding Dependent Variables

Let us now have a look at our dependent variable y.
"""

print(y)

"""Our dependent variable y is also a categorical variable. However in this case we can simply assign 0 and 1 to the two categories ‘No’ and ‘Yes’. In this case, we do not require dummy variables to encode the ‘Predicted’ variable as it is a dependent variable that will not be used to train the model."""

## To code this, we are going to need the LabelEncoder class.

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)
print(y)



"""Feature Scaling – The last step of Feature Engineering

Feature Scaling is the process of scaling or converting all the values in our dataset to a given scale. Some machine learning algorithms like linear regression, logistic regression, etc use gradient descent optimization. Such algorithms require the data to be scaled in order to perform optimally. K Nearest Neighbours, Support Vector Machine, and K-Means clustering also show a drastic rise in performance on scaling the data.

There are two main techniques of feature scaling:

Standardization Normalization
"""



"""Normalization is the process of scaling the data values in such a way that that the value of all the features lies between 0 and 1.



This method works well when the data is normally distributed.

Standardization is the process of scaling the data values in such a way that that they gain the properties of standard normal distribution. This means that the data is rescaled in such a way that the mean becomes zero and the data has unit standard deviation.


Standardized values do not have a fixed bounded range like Normalised values.


Let us have a look at the code. If you do not have separate training and test sets then you can split your dataset into two parts – one for training and the other for testing.
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)
print(X_train)

"""Now we shall import the StandardScaler class to scale all the variables."""